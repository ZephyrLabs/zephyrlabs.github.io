<!DOCTYPE html>
    <link id="theme" rel="stylesheet" 
    type="text/css" href="../../themes/article_dark.css" />
    <link id="theme" rel="stylesheet" 
    type="text/css" href="../../themes/article.css" />

    <script>
        function theme_toggle() {
            var theme = document.getElementsByTagName('link')[0];
            if (document.getElementById("theme_button").innerText == "Dark") {
                document.getElementById("theme_button").innerText = "Light";

                if (theme.getAttribute('href') == "../../themes/article_dark.css") {
                    theme.setAttribute('href', "../../themes/article_light.css");
                }
                
            }
            else {
                document.getElementById("theme_button").innerText = "Dark";
                theme.setAttribute('href', "../../themes/article_dark.css");
            }
        }
    </script>

    <body>        
        <div class="topnav">
            <a href="../../index.html">Home</a>
            <a href="../about/index.html">About</a>
        </div>
        <div>
            <button id="theme_button" onclick="theme_toggle()">Dark</button>
        </div>
        <div>
            <h2 id="toc_1">Getting started with clustering</h2>

            <h3 id="toc_1.1">part 1:</h3>

            <h4 id="toc_1.1.1">Introduction:</h4>

            <p>Its always has been my wish to build my own SBC cluster, and its finally a reality!</p>

            <p>in this article I&#39;ll explain what were my steps to setup and build my own super powered SBC cluster</p>

            <h4 id="toc_1.1.2">Getting started:</h4>

            <p>to build a cluster we need to select our choice of processing media and various components to interconnect the system.</p>

            <p>I chose the VIM3 as the brains of the operation/s</p>

            <p><img src="vim3.jpg" alt="vim3" title="vim3"></p>

            <p>I&#39;ve been using the VIM3 for about 2 years now, and these is nothing as fast that does wonders without breaking a sweat,</p>

            <p>I&#39;ll be using the VIM3s in a 4+1 configuration where 1 VIM3 controls the other 4 VIM3s</p>

            <p>the net resources in this cluster (excuding the master node) is:</p>
            <code>24 cores</code><br>
            <code>12 GB of RAM</code>
            <p></p>this insane amount of resources will allow us to some intense things</p>

            <code>but what is a cluster without the literal fabric that connects it all ?</code>

            <h4 id="toc_1.1.3">The essentials:</h4>

            <p><img src="essentials.png" alt="interconnect" title="essentials"><br>
            <code>The four horsemen of connecting it all</code></p>

            <p>the 4 main things we need for connecting it all together include:</p>

            <li>TPlink 8 port gigabit ethernet switch</li>
            <li>generic 6 port wall charger</li>
            <li>Ethernet cables</li>
            <li>USB-C cables</li>

            <p>the gigabit ethernet switch is the literal bread and butter of the entire operation, allowing high speed communication between all the various nodes connected to it.</p>

            <p>the charger is a branded version of the generic HDS-HDD10T charger, it provides 60w of power distributed among 6 individual USB ports, allowing us to power up the system without any power deficiency</p>

            <p>the cables also play a vital role as they help to connect everything together
            the USB cables were plain braided USB-C to USB-A and the ethernet cables were 1 meter long patch cables which were CAT6 compliant.</p>

            <h4 id="toc_1.1.4">Building the cluster:</h4>

            <p>except the master node, each node was fitted with a small heatsink to help dissipate heat, and was stacked with pairs of M2 standoffs</p>

            <p><img src="stack_up.png" alt="stack_up" title="stacked up node"><br>
            <code>putting it together piece by piece</code></p>

            <p>all such nodes were given standoffs and stacked up to finally give a tower of SBCs</p>

            <p><img src="main_cluster.png" alt="main_cluster" title="main cluster"><br>
            <code>the tower of power</code></p>

            <p>the nodes are well spaced to allow for good airflow during passive cooling.</p>

            <p>the top most node which is the master node which controls all the other<br> 
            nodes is fitted with a passive heatsink from khadas, and acts as the<br> 
            endcap for the standoff heads and also gives the extra necessary cooling<br> 
            for the master node which will have to manage all other devices</p>

            <h4 id="toc_1.1.5">setting up some initial software...</h4>

            <p>for the cluster we need to setup all the nodes to run linux, the simple 
            approach I took to do this seamlessly is using <a href="https://docs.khadas.com/vim3/Krescue.html">Krescue</a></p>

            <p>one node at a time can be fitted with a thumbdrive containing krescue and 
            the necessary firmware file and connected to the computer to open up the 
            krescue web console and flash the firmware</p>

            <p><img src="krescue_web.png" alt="krescue_web" title="krescue web client">
            
            <p>my choice of firmware was the stock Arch image install via krescue scripts,<br>
            running the mainline kernel, this would mean I&#39;d be getting the up to date<br> 
            software and latest optimizations available, kudos to @Artem from khadas<br>
            for creating these amazing scripts!</p>

            <p>Arch linux is very lightweight, fast to boot, and is very vanilla (apart<br> 
            from being a bit difficult to setup if you are an linux beginner)</p>

            <h4 id="toc_1.1.6">cable management:</h4>

            <p>much work was done to organize the extra lengthy cables I used here, if<br> 
            you were to attempt this you could get away with some smaller patch cables<br> 
            for less of a mess.</p>

            <p>extensive usage of twist ties and braiding was done to maintain the<br> 
            seamless flow of the cables</p>

            <h4 id="toc_1.1.7">what&#39;s next ?:</h4>

            <p>in the next part I&#39;ll be exploring some of the software I used to setup<br> 
            the cluster this includes things like passwordless SSH functionality,<br> 
            various software for running code with the power of all the clusters etc. </p>

            <h3 id="toc_1.2">Part 2:</h3>

            <h4 id="toc_1.2.1">setting up the software:</h4>

            <p>our first thing on the agenda of software management will be to make sure<br> 
            we have SSH keys enabled for each of the nodes so that we can have<br>
            passwordless access to each node from the master node.</p>

            <p>we first want to figure out which IP address of the various nodes, but<br>
            since we don&#39;t know one node from another we want to change the host name<br>
            of the cluster on the master node</p>

            <p>we first power up the master node (the one on top with the bigger heatsink,<br> 
            and change the host name, this can be done with<br>
            <code>sudo nano /etc/hostname</code> and changing the default name to whatever we<br> 
            prefer, I named my cluster <strong>orchid</strong> </p>

            <p>after that we can reboot the vim and power up all the nodes and login to<br>
            my router to see the attached devices and their various names, its much<br>
            easier to spot the master node among the other nodes, since I initially<br>
            know the IP address of my master node from previously sshing into it, I<br>
            only need to remember the IP address of the other nodes</p>

            <p>usually I would need to setup a static IP address but since the image was<br>
            preconfigured to set its own IP, I didn&#39;t meddle with that.</p>

            <p>I opened up a corresponding number of SSH terminals to my master node from<br> 
            my main computer equal to the number of other nodes on the network (which<br>
            in my case was 5 other nodes)</p>

            <p>after that I ran <code>ssh-keygen</code> and gave enter without any data input to<br>
            make sure I would have a passwordless entry.</p>

            <p>after that I run <code>ssh-copy-id &lt;ip-of-a-node&gt;</code> for each node on each<br>
            terminal after which I run <code>ssh &lt;ip-of-a-node&gt;</code> on each terminal for each<br> 
            node and run updates (<code>sudo pacman -Syu</code>) to bring all<br>
            the nodes up to date</p>

            <p>we will first install <code>mpich</code> and <code>python3-mpi4py</code> from the pacman package<br> 
            manager (<code>sudo pacman -S mpich python3-mpi4py</code>), this will allow us to<br> 
            run commands simulatneously when needed in the future like for example<br>
            updates and necessary packages etc.</p>

            <p>and just like that the initial software setup is complete for the cluster.<br>
            optionally you can set individual hostnames for each of the various nodes.</p>

            <h4 id="toc_1.2.2">What&#39;s next ?:</h4>

            <p>In the next part I&#39;ll be running some example code to show the cluster<br>
            using its computational resource. we can compare these numbers to show how<br>
            fast it is compared to other various devices like an Individual VIM and<br>
            another Rasberry pi cluster.</p>
        </div>
    </body>
</html>